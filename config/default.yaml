project: "llm"
model_type: qwen3
model_path: Qwen/Qwen3-0.6B
model_name: default
audio_encoder_type: mel_spec
cache_dir: ./cache
steps: 100000
logdir: ./exp
expdir: ${logdir}/${project}/${model_name}
precision: "32" # bf16-mixed
activation_checkpointing: True
model:
  target: mllm.models.trainer.LMTrainingWrapper
  params:
    model_config:
      model_type: ${model_type}
      model_path: ${model_path}
      cache_dir: ${cache_dir}
      audio_encoder_type: ${audio_encoder_type}
      adapter_type: mlp  # linear, mlp, or q_former
      adapter_config:
        i_dim: 128  # mel_spec n_mels output dimension
        o_dim: 1024  # Qwen3-0.6B hidden size
      attn_implementation: sdpa
      precision: ${precision}
      apply_lora: True
      lora_rank: 128
      max_length: 2048  # 1001 + 512 + 16
      input_length: 16
      output_length: 512
      save_adapter: True
      expdir: ${expdir}
    optimizer_config:
      target: torch.optim.AdamW
      params:
        betas: [0.9, 0.99]
        lr: 5e-5
        fused: True
    scheduler_config:
      target: mllm.common_utils.scheduler.LinearWarmupCosineDecay
      params:
        warmup_steps: 10000
        total_steps: ${steps}
data:
  target: mllm.models.data.pl_modules.AudioTextDataModule
  params:
    data_dir: ./data  # Update this path
    model_path: ${model_path}
    cache_dir: ${cache_dir}
    batch_size: 4
    num_workers: 8
lightning:
  logdir: ${logdir}
  logger_type: tensorboard
  version_name: ${model_name}
  callbacks:
    save_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        dirpath: ${expdir}
        monitor: val_loss
        mode: min
        filename: "{epoch}-{step}-{val_loss:.2f}"
        save_last: True
        every_n_train_steps: 10000
    learning_rate:
      target: pytorch_lightning.callbacks.LearningRateMonitor
      params:
        logging_interval: step
  trainer:
    benchmark: True
    accelerator: gpu
    strategy: ddp # fsdp
    max_steps: ${steps}
    log_every_n_steps: 100
    precision: ${precision}
    check_val_every_n_epoch: 1
    num_sanity_val_steps: 1
  tf32: True
